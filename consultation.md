# Machine Learning Consultation

## Neural Network
- Every Layer learns something else
- Stack functions into each layer


## Chi Square
Mutual information would think negative features are small
Since they have weights, rarer features get ignored

Squaring the smaller weights would flesh them out even though they might have negative impact on regular mutual information

## Mutual Information
	a    \a    total
c   2850  2150 5000	
\c  3150  1850 5000
	6000  4000 10000

Mutual information would find this predictive while chi2 might not
Not very useful


## EM Method
Only a way of describing methods


## Gradient Descent/ Logistic Regression
Logistic Regression has the Sigmoid function
Associate every values with weights
Initialise beta to random values at the start
_Yi would always either be 1 or 0_


If training instance is 1, the RHS is irrelevant 


Gradient ascent: Definition of 'goodness'
- Finds max
- Points into direction of increase and solution
- For each of the beta values, imagine them to have values


Gradient descent: Definition of errors
- Points into the direction of increase
- Points away from the solution
- Choose error function once that 

